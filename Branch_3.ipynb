{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGkNK/Km5zbF+3W9yVDcA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eileenliu953/NHERI-AVEE-2023/blob/main/Branch_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QMfrryFpNc0"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install markdown2\n",
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pyarrow\n",
        "import markdown2\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import GPT2TokenizerFast\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from openai.embeddings_utils import cosine_similarity\n",
        "\n",
        "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "openai.api_key = ''\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    \"\"\"count the number of tokens in a string\"\"\"\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "def break_text(text, max_tokens):\n",
        "  tokens = text.split()\n",
        "  chunks = []\n",
        "  current_chunk = \"\"\n",
        "\n",
        "  for token in tokens:\n",
        "      if len(current_chunk) + len(token) < max_tokens:\n",
        "          current_chunk += token + \" \"\n",
        "      else:\n",
        "          chunks.append(current_chunk.strip())\n",
        "          current_chunk = token + \" \"\n",
        "\n",
        "  if current_chunk:\n",
        "      chunks.append(current_chunk.strip())\n",
        "\n",
        "  return chunks\n",
        "\n",
        "\n",
        "\n",
        "#directory = \"/content/drive/MyDrive/Technical Documentation \" #might need to be input value\n",
        "\n",
        "def reading_files(directory_path):\n",
        "  data = []\n",
        "  df_f = pd.DataFrame(data, columns=[\"file\", \"heading\", \"content\", \"tokens\"])   #creating empty data frame\n",
        "\n",
        "  #iterating through each file to tokenize\n",
        "  for i in os.listdir(directory_path):\n",
        "        with open(f\"{i}\", \"r\") as file:\n",
        "            content = file.read()\n",
        "        file_no_md = i.replace(\".md\", \"\")\n",
        "        file_no_md = \"tapipy/\" + file_no_md\n",
        "        html = markdown2.markdown(content)    # Use markdown2 to convert the markdown file to html\n",
        "        soup = BeautifulSoup(html, \"html.parser\")   # Use BeautifulSoup to parse the html\n",
        "\n",
        "        # Initialize variables to store heading, subheading, and corresponding paragraphs\n",
        "        headings = []\n",
        "        paragraphs = []\n",
        "        data = []\n",
        "        MAX_WORDS = 500\n",
        "\n",
        "        # Iterate through the tags in the soup\n",
        "        for tag in soup.descendants:\n",
        "            # Check if the tag is a heading\n",
        "            if tag.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
        "                # When the next heading is encountered, print the heading, subheading, and corresponding paragraphs\n",
        "                if headings and paragraphs:\n",
        "                    hdgs = \" \".join(headings)\n",
        "                    para = \" \".join(paragraphs)\n",
        "                    counting = count_tokens(para)\n",
        "                    if counting > 1024:\n",
        "                      para_chunks = break_text(para, 1024)\n",
        "                      for chunk in para_chunks:\n",
        "                        data.append([file_no_md, hdgs, chunk, count_tokens(chunk)])\n",
        "                        headings = []\n",
        "                        paragraphs = []\n",
        "                    else:\n",
        "                      data.append([file_no_md, hdgs, para, count_tokens(para)])\n",
        "                      headings = []\n",
        "                      paragraphs = []\n",
        "                # Add to heading\n",
        "                headings.append(tag.text)\n",
        "            # Check if the tag is a paragraph\n",
        "            elif tag.name == \"p\":\n",
        "                paragraphs.append(tag.text)\n",
        "\n",
        "        # creating data frame\n",
        "\n",
        "        df = pd.DataFrame(data, columns=[\"file\", \"heading\", \"content\", \"tokens\"])\n",
        "        df = df[df.tokens > 40]\n",
        "        df = df.reset_index().drop('index',axis=1) # reset index\n",
        "        df_f = pd.concat([df_f, df])\n",
        "        df_f = df_f.reset_index().drop('index',axis=1)\n",
        "\n",
        "  return df_f\n",
        "\n",
        "\n",
        "def get_embedding(text: str, model: str=EMBEDDING_MODEL):\n",
        "    result = openai.Embedding.create(\n",
        "      model=model,\n",
        "      input=text\n",
        "    )\n",
        "    return result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "\n",
        "def compute_doc_embeddings(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
        "\n",
        "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
        "    }\n",
        "\n",
        "\n",
        "def order_documents_query_similarity(data, query_str, nres=3):\n",
        "    embedding = get_embedding(query_str, model=EMBEDDING_MODEL)\n",
        "    data['similarities'] = data.vector_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
        "\n",
        "    res = data.sort_values('similarities', ascending=False).head(nres)\n",
        "    return res\n",
        "\n",
        "\n",
        "def construct_prompt(question: str, df: pd.DataFrame, ncontents = 3) -> str:\n",
        "    \"\"\"\n",
        "    Fetch relevant\n",
        "    \"\"\"\n",
        "    most_relevant_document_sections = order_documents_query_similarity(df, question)\n",
        "\n",
        "    chosen_sections = []\n",
        "    chosen_section_len = 0\n",
        "\n",
        "    MAX_SECTION_LEN = 500\n",
        "    context = order_documents_query_similarity(df, question)\n",
        "    context.head()\n",
        "\n",
        "    for _, ctx in context.iterrows():\n",
        "        chosen_section_len += ctx.tokens\n",
        "        if chosen_section_len > MAX_SECTION_LEN:\n",
        "            break\n",
        "\n",
        "        chosen_sections.append(\" \" + ctx.content.replace(\"\\n\", \" \"))\n",
        "\n",
        "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
        "\n",
        "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
        "\n",
        "\n",
        "COMPLETIONS_API_PARAMS = {\n",
        "    \"temperature\": 0.0,\n",
        "    \"max_tokens\": 300,\n",
        "    \"model\": COMPLETIONS_MODEL,\n",
        "}\n",
        "\n",
        "def answer_query_with_context(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    show_prompt: bool = False) -> str:\n",
        "\n",
        "    prompt = construct_prompt(\n",
        "        query,\n",
        "        df\n",
        "    )\n",
        "\n",
        "    if show_prompt:\n",
        "        print(prompt)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "                prompt=prompt,\n",
        "                **COMPLETIONS_API_PARAMS\n",
        "            )\n",
        "\n",
        "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "\n",
        "\n",
        "#calling functions\n",
        "df_f = reading_files(\"/content/drive/MyDrive/Technical Documentation \")\n",
        "vector_embedding = compute_doc_embeddings(df_f)\n",
        "df_f['vector_embedding'] = pd.Series(vector_embedding)\n",
        "#df_f #this will show the results\n",
        "\n",
        "\n",
        "# \"What is the purpose of tapis\" should be an input that the user controls. The input code hasn't been written yet.\n",
        "construct_prompt(question=\"What is the purpose of tapis?\", df=df_f)\n",
        "res = order_documents_query_similarity(df_f, \"What is the purpose of tapis?\") #input value\n",
        "answer_query_with_context(\"What is the purpose of tapis?\", df_f)"
      ]
    }
  ]
}